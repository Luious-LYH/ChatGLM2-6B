{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\developer\\Anaconda\\envs\\ChatGLM2-6B\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Loading checkpoint shards: 100%|██████████| 7/7 [00:58<00:00,  8.43s/it]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Library cudart is not initialized",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      4\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./chatglm2-6b\u001b[39m\u001b[38;5;124m'\u001b[39m, trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./chatglm2-6b\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m      8\u001b[0m     model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[1;32m~/.cache\\huggingface\\modules\\transformers_modules\\local\\modeling_chatglm.py:1197\u001b[0m, in \u001b[0;36mChatGLMForConditionalGeneration.quantize\u001b[1;34m(self, bits, empty_init, device, **kwargs)\u001b[0m\n\u001b[0;32m   1193\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquantized \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mquantization_bit \u001b[38;5;241m=\u001b[39m bits\n\u001b[1;32m-> 1197\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mencoder \u001b[38;5;241m=\u001b[39m \u001b[43mquantize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mempty_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mempty_init\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1198\u001b[0m \u001b[43m                                    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1199\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~/.cache\\huggingface\\modules\\transformers_modules\\local\\quantization.py:155\u001b[0m, in \u001b[0;36mquantize\u001b[1;34m(model, weight_bit_width, empty_init, device)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Replace fp16 linear with quantized linear\"\"\"\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m--> 155\u001b[0m     layer\u001b[38;5;241m.\u001b[39mself_attention\u001b[38;5;241m.\u001b[39mquery_key_value \u001b[38;5;241m=\u001b[39m \u001b[43mQuantizedLinear\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_bit_width\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_bit_width\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attention\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery_key_value\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attention\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery_key_value\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attention\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery_key_value\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attention\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery_key_value\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mempty_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mempty_init\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    163\u001b[0m     layer\u001b[38;5;241m.\u001b[39mself_attention\u001b[38;5;241m.\u001b[39mdense \u001b[38;5;241m=\u001b[39m QuantizedLinear(\n\u001b[0;32m    164\u001b[0m         weight_bit_width\u001b[38;5;241m=\u001b[39mweight_bit_width,\n\u001b[0;32m    165\u001b[0m         weight\u001b[38;5;241m=\u001b[39mlayer\u001b[38;5;241m.\u001b[39mself_attention\u001b[38;5;241m.\u001b[39mdense\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mcurrent_device()),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    169\u001b[0m         empty_init\u001b[38;5;241m=\u001b[39mempty_init\n\u001b[0;32m    170\u001b[0m     )\n\u001b[0;32m    171\u001b[0m     layer\u001b[38;5;241m.\u001b[39mmlp\u001b[38;5;241m.\u001b[39mdense_h_to_4h \u001b[38;5;241m=\u001b[39m QuantizedLinear(\n\u001b[0;32m    172\u001b[0m         weight_bit_width\u001b[38;5;241m=\u001b[39mweight_bit_width,\n\u001b[0;32m    173\u001b[0m         weight\u001b[38;5;241m=\u001b[39mlayer\u001b[38;5;241m.\u001b[39mmlp\u001b[38;5;241m.\u001b[39mdense_h_to_4h\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mcurrent_device()),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    177\u001b[0m         empty_init\u001b[38;5;241m=\u001b[39mempty_init\n\u001b[0;32m    178\u001b[0m     )\n",
      "File \u001b[1;32m~/.cache\\huggingface\\modules\\transformers_modules\\local\\quantization.py:139\u001b[0m, in \u001b[0;36mQuantizedLinear.__init__\u001b[1;34m(self, weight_bit_width, weight, bias, device, dtype, empty_init, *args, **kwargs)\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mround(weight \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight_scale[:, \u001b[38;5;28;01mNone\u001b[39;00m])\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mint8)\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m weight_bit_width \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m:\n\u001b[1;32m--> 139\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m \u001b[43mcompress_int4_weight\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m Parameter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mto(device), requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight_scale \u001b[38;5;241m=\u001b[39m Parameter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight_scale\u001b[38;5;241m.\u001b[39mto(device), requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~/.cache\\huggingface\\modules\\transformers_modules\\local\\quantization.py:78\u001b[0m, in \u001b[0;36mcompress_int4_weight\u001b[1;34m(weight)\u001b[0m\n\u001b[0;32m     75\u001b[0m gridDim \u001b[38;5;241m=\u001b[39m (n, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     76\u001b[0m blockDim \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mmin\u001b[39m(round_up(m, \u001b[38;5;241m32\u001b[39m), \u001b[38;5;241m1024\u001b[39m), \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 78\u001b[0m \u001b[43mkernels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint4WeightCompression\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgridDim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43mblockDim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_void_p\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_ptr\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_void_p\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_ptr\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int32\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int32\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     84\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mf:\\developer\\Anaconda\\envs\\ChatGLM2-6B\\lib\\site-packages\\cpm_kernels\\kernels\\base.py:48\u001b[0m, in \u001b[0;36mKernelFunction.__call__\u001b[1;34m(self, gridDim, blockDim, sharedMemBytes, stream, params)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(gridDim) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(blockDim) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m---> 48\u001b[0m func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m cuda\u001b[38;5;241m.\u001b[39mcuLaunchKernel(func, \n\u001b[0;32m     51\u001b[0m     gridDim[\u001b[38;5;241m0\u001b[39m], gridDim[\u001b[38;5;241m1\u001b[39m], gridDim[\u001b[38;5;241m2\u001b[39m], \n\u001b[0;32m     52\u001b[0m     blockDim[\u001b[38;5;241m0\u001b[39m], blockDim[\u001b[38;5;241m1\u001b[39m], blockDim[\u001b[38;5;241m2\u001b[39m], \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     55\u001b[0m     ]\n\u001b[0;32m     56\u001b[0m )\n",
      "File \u001b[1;32mf:\\developer\\Anaconda\\envs\\ChatGLM2-6B\\lib\\site-packages\\cpm_kernels\\kernels\\base.py:36\u001b[0m, in \u001b[0;36mKernelFunction._prepare_func\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_prepare_func\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m---> 36\u001b[0m     curr_device \u001b[38;5;241m=\u001b[39m \u001b[43mcudart\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudaGetDevice\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m     cudart\u001b[38;5;241m.\u001b[39mcudaSetDevice(curr_device)   \u001b[38;5;66;03m# ensure cudart context\u001b[39;00m\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m curr_device \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_funcs:\n",
      "File \u001b[1;32mf:\\developer\\Anaconda\\envs\\ChatGLM2-6B\\lib\\site-packages\\cpm_kernels\\library\\base.py:72\u001b[0m, in \u001b[0;36mLib.bind.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 72\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLibrary \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is not initialized\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__name)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Library cudart is not initialized"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('./chatglm2-6b', trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained('./chatglm2-6b', trust_remote_code=True).quantize(4).cuda()\n",
    "\n",
    "with torch.no_grad():\n",
    "    model = model.eval()\n",
    "    response, history = model.chat(tokenizer, \"你是谁？\", history=[])\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\developer\\Anaconda\\envs\\ChatGLM2-6B\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Loading checkpoint shards: 100%|██████████| 7/7 [01:04<00:00,  9.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我是一个名为 ChatGLM2-6B 的人工智能助手，是基于清华大学 KEG 实验室和智谱 AI 公司于 2023 年共同训练的语言模型开发的。我的任务是针对用户的问题和要求提供适当的答复和支持。\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# 初始化CUDA\n",
    "torch.cuda.init()\n",
    "\n",
    "# 加载模型\n",
    "tokenizer = AutoTokenizer.from_pretrained('./chatglm2-6b', trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained('./chatglm2-6b', trust_remote_code=True).quantize(4)\n",
    "\n",
    "# 将模型移动到GPU上\n",
    "model = model.cuda()\n",
    "\n",
    "with torch.no_grad():\n",
    "    model = model.eval()\n",
    "    response, history = model.chat(tokenizer, \"你是谁？\", history=[])\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember to replace the proxy IP and port with your own. If you're not sure about these, please check with your network administrator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch版本： 2.1.0+cu118\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "pytorch_version = torch.__version__\n",
    "print(\"PyTorch版本：\", pytorch_version)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ChatGLM2-6B",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
